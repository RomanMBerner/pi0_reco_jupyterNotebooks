{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pi0 Reconstruction Chain\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'notebook_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4fc7c5e2c62c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnotebook_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdraw_training_curves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m labels = {'uresnet':'Segmentation',\n\u001b[1;32m      4\u001b[0m           \u001b[0;34m'seg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Segmentation'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0;34m'ppn'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'PPN'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'notebook_utils'"
     ]
    }
   ],
   "source": [
    "from notebook_utils import draw_training_curves\n",
    "\n",
    "labels = {'uresnet':'Segmentation',\n",
    "          'seg':'Segmentation',\n",
    "          'ppn':'PPN',\n",
    "          'ppn1':'PPN 1',\n",
    "          'ppn2':'PPN 2',\n",
    "          'ppn_type':'PPN Type',\n",
    "          'type':'PPN Type',\n",
    "          'class':'PPN Class',\n",
    "          'distance':'PPN Distance',\n",
    "          'shower_node':'Shower Primary',\n",
    "          'shower_edge':'Shower Aggregation',\n",
    "          'track_edge':'Track Aggregation',\n",
    "          'particle_node':'Particle Primary',\n",
    "          'particle_edge':'Particle Aggregation'}\n",
    "\n",
    "#keys = ['seg', 'ppn']\n",
    "# keys = ['ppn']\n",
    "# labels_acc = {k+'_acc':labels[k] for k in keys}\n",
    "# #labels_acc.update({'acc':'Total'})\n",
    "# draw_training_curves(log_dir = 'train/log/',\n",
    "#                      models = ['wolcott_up','wolcott_fc','wolcott_fc2'],\n",
    "#                      metrics = [k+'_acc' for k in keys],\n",
    "#                      names = labels_acc,\n",
    "#                      limits = [0.,1],\n",
    "#                      max_iter=-1, smoothing=10, same_canvas=True, paper=False, iter_per_epoch=4*980)\n",
    "\n",
    "# draw_training_curves(log_dir = 'train/log/',\n",
    "#                      models = ['grappa_shower','grappa_shower_cpu','grappa_track','grappa_track_cpu','grappa_inter','grappa_inter_cpu'],\n",
    "#                      metrics = ['titer'],\n",
    "#                      names = {'grappa_shower':'Shower','grappa_shower_cpu':'Shower CPU','grappa_track':'Track','grappa_track_cpu':'Track CPU','grappa_inter':'Interaction','grappa_inter_cpu':'Interaction CPU'},\n",
    "#                      limits = [0.,10],\n",
    "#                      max_iter=-1, smoothing=10, same_canvas=True, paper=False, iter_per_epoch=980)\n",
    "\n",
    "keys = ['seg', 'ppn', 'shower_node', 'shower_edge']\n",
    "labels_acc = {k+'_accuracy':labels[k] for k in keys}\n",
    "labels_acc.update({'accuracy':'Total'})\n",
    "draw_training_curves(log_dir = 'train/log/',\n",
    "                     models = ['full_pid_v02'],\n",
    "                     metrics = ['accuracy'] + [k+'_accuracy' for k in keys],\n",
    "                     names = labels_acc,\n",
    "                     limits = [0.9,1],\n",
    "                     max_iter=-1, smoothing=10, same_canvas=True, paper=False, iter_per_epoch=980)\n",
    "\n",
    "keys = ['seg', 'ppn', 'track_edge']\n",
    "labels_acc = {k+'_accuracy':labels[k] for k in keys}\n",
    "labels_acc.update({'accuracy':'Total'})\n",
    "draw_training_curves(log_dir = 'train/log/',\n",
    "                     models = ['full_pid_v02_track'],\n",
    "                     metrics = ['accuracy'] + [k+'_accuracy' for k in keys],\n",
    "                     names = labels_acc,\n",
    "                     limits = [0.9,1],\n",
    "                     max_iter=-1, smoothing=10, same_canvas=True, paper=False, iter_per_epoch=980)\n",
    "\n",
    "keys = ['seg', 'ppn', 'particle_node', 'particle_edge']\n",
    "labels_acc = {k+'_accuracy':labels[k] for k in keys}\n",
    "labels_acc.update({'accuracy':'Total'})\n",
    "draw_training_curves(log_dir = 'train/log/',\n",
    "                     models = ['full_pid_v03'],\n",
    "                     metrics = ['accuracy'] + [k+'_accuracy' for k in keys],\n",
    "                     names = labels_acc,\n",
    "                     limits = [0.9,1],\n",
    "                     max_iter=-1, smoothing=10, same_canvas=True, paper=False, iter_per_epoch=980)\n",
    "\n",
    "labels_acc = {k+'_loss':labels[k] for k in keys}\n",
    "labels_acc.update({'loss':'Total'})\n",
    "draw_training_curves(log_dir = 'train/log/',\n",
    "                     models = ['full_pid_v03'],\n",
    "                     metrics = ['loss'] + [k+'_loss' for k in keys],\n",
    "                     names = labels_acc,\n",
    "                     limits = [0.001,1],\n",
    "                     max_iter=-1, smoothing=10, same_canvas=True, paper=False, iter_per_epoch=980, log=True)\n",
    "\n",
    "# keys = ['seg', 'ppn', 'shower_node', 'shower_edge', 'inter_edge']\n",
    "# labels = {'seg':'Segmentation', 'ppn':'PPN', 'shower_edge':'Shower clustering', 'shower_node':'Shower primary', 'inter_edge':'Interaction clustering'}\n",
    "# labels_acc = {k+'_accuracy':labels[k] for k in keys}\n",
    "# labels_acc.update({'accuracy':'Total'})\n",
    "# draw_training_curves(log_dir = 'train/log/',\n",
    "#                      models = ['full'],\n",
    "#                      metrics = ['accuracy'] + [k+'_accuracy' for k in keys],\n",
    "#                      names = labels_acc,\n",
    "#                      limits = [0.85,1],\n",
    "#                      max_iter=-1, smoothing=100, same_canvas=True, paper=False)\n",
    "\n",
    "# keys = ['seg', 'ppn', 'shower_node', 'shower_edge', 'inter_node', 'inter_edge']\n",
    "# labels = {'seg':'Segmentation', 'ppn':'PPN', 'shower_edge':'Shower clustering', 'shower_node':'Shower primary', 'inter_node':'Particle ID', 'inter_edge':'Interaction clustering'}\n",
    "# labels_acc = {k+'_accuracy':labels[k] for k in keys}\n",
    "# labels_acc.update({'accuracy':'Total'})\n",
    "# draw_training_curves(log_dir = 'train/log/',\n",
    "#                      models = ['full_pid'],\n",
    "#                      metrics = ['accuracy'] + [k+'_accuracy' for k in keys],\n",
    "#                      names = labels_acc,\n",
    "#                      limits = [0.85,1],\n",
    "#                      max_iter=-1, smoothing=100, same_canvas=True, paper=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the configuration, prepare handlers, load an event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'notebook_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-dc6231339e6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Define the data loader configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnotebook_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_inference_cfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# cfg = get_inference_cfg(cfg_path = 'train/pi0_shower_only_v01.cfg', batch_size=1) # Includes interaction clustering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# cfg['trainval']['model_path'] = 'train/weights/shower_only/snapshot-106999.ckpt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'notebook_utils'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"lartpc_mlreco3d\")\n",
    "\n",
    "import yaml\n",
    "from mlreco.main_funcs import process_config, prepare\n",
    "\n",
    "# Define the data loader configuration\n",
    "from notebook_utils import get_inference_cfg\n",
    "# cfg = get_inference_cfg(cfg_path = 'train/pi0_shower_only_v01.cfg', batch_size=1) # Includes interaction clustering\n",
    "# cfg['trainval']['model_path'] = 'train/weights/shower_only/snapshot-106999.ckpt'\n",
    "# cfg = get_inference_cfg(cfg_path = 'train/pi0_full_v01.cfg', batch_size=1) # Includes interaction clustering\n",
    "# cfg['trainval']['model_path'] = 'train/weights/full/snapshot-100999.ckpt'\n",
    "cfg = get_inference_cfg(cfg_path = 'train/pi0_full_pid_v02.cfg', batch_size=1) # Includes interaction clustering\n",
    "cfg['trainval']['model_path'] = 'train/weights/full_pid_v02/snapshot-34999.ckpt'\n",
    "# cfg = get_inference_cfg(cfg_path = 'train/pi0_full_pid_v03.cfg', batch_size=1) # Includes interaction clustering\n",
    "# cfg['trainval']['model_path'] = 'train/weights/full_pid_v03/snapshot-22999.ckpt'\n",
    "# cfg = get_inference_cfg(cfg_path = 'train/config.train.fullchain.yaml', batch_size=1) # Includes interaction clustering\n",
    "# cfg['trainval']['model_path'] = 'train/weights/wolcott/snapshot-9699.ckpt'\n",
    "cfg['iotool']['dataset']['data_keys'] = ['/home/frans/slac/dlp/data/pi0_dunend_v2_p00.root']\n",
    "# cfg['iotool']['dataset']['data_keys'] = ['/home/frans/slac/dlp/data/nd.fhc.0.supera.voxpitch=4mm.nothresh.root']\n",
    "cfg['iotool']['dataset']['schema']['particles'] = ['parse_particle_asis', 'particle_pcluster', 'cluster3d_pcluster']\n",
    "#cfg['iotool']['dataset']['schema']['blank'] = ['parse_sparse3d_scn_binary', 'sparse3d_pcluster']\n",
    "\n",
    "# Pre-process configuration\n",
    "process_config(cfg)\n",
    "\n",
    "# Instantiate \"handlers\n",
    "hs = prepare(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlreco.main_funcs import apply_event_filter\n",
    "import numpy as np\n",
    "# Bizarre GT in event 200 ? Two very non-conlinerar far apart fragments supposedly in the same group ?\n",
    "event_id = [48]\n",
    "apply_event_filter(hs, event_id)\n",
    "data, output = hs.trainer.forward(hs.data_io_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set plotly style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlreco'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-06bd9761a0c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_objs\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0miplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmlreco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoints\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscatter_points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlreco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetwork_topology\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmlreco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly_layout3d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mlreco'"
     ]
    }
   ],
   "source": [
    "# Create a continuous color scale from the list of colors (values span from 0 to 1)\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot\n",
    "from mlreco.visualization.points import scatter_points\n",
    "from mlreco.visualization.gnn import network_topology\n",
    "from mlreco.visualization import plotly_layout3d\n",
    "\n",
    "plotly_colors = [\n",
    "    '#1f77b4',  # muted blue\n",
    "    '#ff7f0e',  # safety orange\n",
    "    '#2ca02c',  # cooked asparagus green\n",
    "    '#d62728',  # brick red\n",
    "    '#9467bd',  # muted purple\n",
    "    '#8c564b',  # chestnut brown\n",
    "    '#e377c2',  # raspberry yogurt pink\n",
    "    '#7f7f7f',  # middle gray\n",
    "    '#bcbd22',  # curry yellow-green\n",
    "    '#17becf'   # blue-teal\n",
    "]\n",
    "\n",
    "\n",
    "def get_colorscale():\n",
    "    colorscale = []\n",
    "    step = 1./48\n",
    "    for i, c in enumerate(px.colors.qualitative.Dark24):\n",
    "        colorscale.append([i*step, c])\n",
    "        colorscale.append([(i+1)*step, c])\n",
    "    for i, c in enumerate(px.colors.qualitative.Light24):\n",
    "        colorscale.append([(i+24)*step, c])\n",
    "        colorscale.append([(i+25)*step, c])\n",
    "    return colorscale\n",
    "\n",
    "layout = plotly_layout3d()\n",
    "bg_color = 'rgb(240,240,240)'\n",
    "layout['scene']['xaxis'] = dict(backgroundcolor=bg_color)\n",
    "layout['scene']['yaxis'] = dict(backgroundcolor=bg_color)\n",
    "layout['scene']['zaxis'] = dict(backgroundcolor=bg_color)\n",
    "layout['showlegend'] = True\n",
    "layout['legend_itemsizing'] = 'constant'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the truth information about the event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_particles(cluster_label, particles):\n",
    "    # Get a color palette, can support up to 48 particles before running having to circle back\n",
    "    import numpy as np\n",
    "    import plotly.express as px\n",
    "    colors = np.concatenate((px.colors.qualitative.Dark24, px.colors.qualitative.Light24))\n",
    "    \n",
    "    # Initialize one graph per particle\n",
    "    from mlreco.visualization.points import scatter_points\n",
    "    graphs = []\n",
    "    for i in range(len(particles)):\n",
    "        # Get a mask that corresponds to the particle entry\n",
    "        mask = cluster_label[:,5] == i\n",
    "        if not np.sum(mask):\n",
    "            continue\n",
    "            \n",
    "        # Initialize the information string\n",
    "        p = particles[i]\n",
    "        start = p.first_step().x(), p.first_step().y(), p.first_step().z()\n",
    "        label = f'Particle {i}'\n",
    "        hovertext = f'Particle type: {p.pdg_code()}<br>Parent type: {p.parent_pdg_code()}<br>Ancestor Type: {p.ancestor_pdg_code()}<br>Group ID: {p.group_id()}<br>Creation: {p.creation_process()}<br>Energy: {p.energy_init():0.1f} MeV<br>Start: ({start[0]:0.1f},{start[1]:0.1f},{start[2]:0.1f})'\n",
    "        \n",
    "        # Initialize the scatter plot\n",
    "        graph = scatter_points(cluster_label[mask,:3], color=colors[i%len(colors)],hovertext=hovertext, cmin=0, cmax=len(colors), markersize=3)[0]\n",
    "        graph['name'] = label\n",
    "        \n",
    "        #'Cluster ID: %d<br>Cluster label: %0.3f<br>Centroid: (%0.1f, %0.1f, %0.1f)'\n",
    "        graphs.append(graph)\n",
    "        \n",
    "    return graphs\n",
    "\n",
    "\n",
    "graphs = scatter_particles(data['cluster_label'][0], data['particles'][0])\n",
    "iplot(go.Figure(graphs, layout=layout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show event display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from mlreco.utils.ppn import uresnet_ppn_type_point_selector\n",
    "from mlreco.utils.gnn.cluster import get_cluster_label\n",
    "from mlreco.utils.gnn.evaluation import edge_assignment_score\n",
    "\n",
    "entry = 0\n",
    "\n",
    "# Energy\n",
    "#print(data['blank'][entry])\n",
    "voxels = data['input_data'][entry][:,:3]\n",
    "energies = data['input_data'][entry][:,-1]\n",
    "graphs = scatter_points(voxels, color=energies, colorscale='Inferno', markersize=3, cmax=2)\n",
    "graphs[-1]['name'] = 'Energy'\n",
    "\n",
    "# Semantics\n",
    "semantics_label = data['cluster_label'][entry][:,-1]\n",
    "graphs += scatter_points(voxels, color=semantics_label, colorscale=get_colorscale(), markersize=3, cmin=0, cmax=4)\n",
    "graphs[-1]['name'] = 'Semantics (Label)'\n",
    "\n",
    "semantics_reco  = np.argmax(output['segmentation'][entry], axis=1)\n",
    "graphs += scatter_points(voxels, color=semantics_reco, colorscale=get_colorscale(), markersize=3, cmin=0, cmax=4)\n",
    "graphs[-1]['name'] = 'Semantics (Reco)'\n",
    "\n",
    "# PPN\n",
    "points = data['particles_label'][entry][:,:3]\n",
    "point_types = data['particles_label'][entry][:,4]\n",
    "graphs += scatter_points(points, color=point_types, colorscale=get_colorscale(), markersize=10, cmin=0, cmax=4)\n",
    "graphs[-1]['name'] = 'PPN points (Label)'\n",
    "\n",
    "points = uresnet_ppn_type_point_selector(data['input_data'][entry], output, score_threshold=0.9, type_score_threshold=0.,\n",
    "                                          type_threshold=1.999, entry=0, score_pool='max', enforce_type=True) # HIGH THRESHOLD\n",
    "point_types  = points[:,-1]\n",
    "graphs += scatter_points(points, color=point_types, colorscale=get_colorscale(), markersize=10, cmin=0, cmax=4)\n",
    "graphs[-1]['name'] = 'PPN points (Reco)'\n",
    "\n",
    "point_scores = softmax(output['points'][entry][:,3:5],axis=1)[:,1]\n",
    "graphs += scatter_points(voxels, color=point_scores, colorscale='Inferno', markersize=3, cmin=0, cmax=1)\n",
    "graphs[-1]['name'] = 'PPN scores (Reco)'\n",
    "\n",
    "point_dists  = np.max(output['points'][entry][:,:3], axis=1)\n",
    "graphs += scatter_points(voxels, color=point_dists, colorscale='Inferno', markersize=3, cmin=0, cmax=5)\n",
    "graphs[-1]['name'] = 'PPN distances (Reco)'\n",
    "\n",
    "# Fragments\n",
    "frag_label = np.unique(data['cluster_label'][entry][semantics_label<4,5], return_inverse=True)[1]\n",
    "graphs += scatter_points(voxels[semantics_label<4], color=frag_label, colorscale=get_colorscale(), markersize=3)\n",
    "graphs[-1]['name'] = 'Fragments (Label)'\n",
    "\n",
    "fragments = output['fragments'][entry]\n",
    "graphs += network_topology(voxels, fragments, clust_labels=np.arange(len(fragments)), markersize=3, colorscale=get_colorscale())\n",
    "graphs[-1]['name'] = 'Fragments (Reco)'\n",
    "\n",
    "# Shower groups\n",
    "if 'shower_edge_pred' in output:\n",
    "    clusts         = output['shower_fragments'][entry]\n",
    "    edge_index     = output['shower_edge_index'][entry]\n",
    "    group_label    = np.unique(get_cluster_label(torch.tensor(data['cluster_label'][entry]), clusts, column=6), return_inverse=True)[1]\n",
    "    edge_label     = group_label[edge_index[:,0]] == group_label[edge_index[:,1]]\n",
    "\n",
    "    shower_graphs = network_topology(voxels, clusts, edge_index[edge_label], clust_labels=group_label, colorscale=get_colorscale())\n",
    "    shower_graphs[0]['name'] = 'Shower groups (Label)'\n",
    "    if len(shower_graphs) == 2:\n",
    "        shower_graphs[1]['name'] = 'Shower edges (Label)'\n",
    "    graphs += shower_graphs\n",
    "\n",
    "    edge_scores    = softmax(output['shower_edge_pred'][entry], axis=1)[:,1]\n",
    "    edge_mask      = edge_scores > 0.5\n",
    "    edge_pred, _   = edge_assignment_score(edge_index, output['shower_edge_pred'][entry], len(clusts))\n",
    "    primary_scores = softmax(output['shower_node_pred'][entry], axis=1)[:,1]\n",
    "    group_reco     = output['shower_group_pred'][entry]\n",
    "\n",
    "    graphs += network_topology(voxels, clusts, clust_labels=primary_scores, colorscale='Portland', cmin=0, cmax=1)\n",
    "    graphs[-1]['name'] = 'Shower primaries (Reco)'\n",
    "\n",
    "    shower_graphs = network_topology(voxels, clusts, edge_index[edge_mask], clust_labels=group_reco, edge_labels=edge_scores[edge_mask], colorscale=get_colorscale())\n",
    "    shower_graphs[0]['name'] = 'Showers (Reco)'\n",
    "    if len(shower_graphs) == 2:\n",
    "        shower_graphs[1]['name'] = 'Shower edges (Reco)'\n",
    "    graphs += shower_graphs\n",
    "    \n",
    "# Track groups\n",
    "if 'track_edge_pred' in output:\n",
    "    clusts         = output['track_fragments'][entry]\n",
    "    edge_index     = output['track_edge_index'][entry]\n",
    "    group_label    = np.unique(get_cluster_label(torch.tensor(data['cluster_label'][entry]), clusts, column=6), return_inverse=True)[1]\n",
    "    edge_label     = group_label[edge_index[:,0]] == group_label[edge_index[:,1]]\n",
    "\n",
    "    track_graphs = network_topology(voxels, clusts, edge_index[edge_label], clust_labels=group_label, colorscale=get_colorscale())\n",
    "    track_graphs[0]['name'] = 'Track groups (Label)'\n",
    "    if len(track_graphs) == 2:\n",
    "        track_graphs[1]['name'] = 'Track edges (Label)'\n",
    "    graphs += track_graphs\n",
    "\n",
    "    edge_scores    = softmax(output['track_edge_pred'][entry], axis=1)[:,1]\n",
    "    edge_mask      = edge_scores > 0.5\n",
    "    edge_pred, _   = edge_assignment_score(edge_index, output['track_edge_pred'][entry], len(clusts))\n",
    "    primary_scores = softmax(output['track_node_pred'][entry], axis=1)[:,1]\n",
    "    group_reco     = output['track_group_pred'][entry]\n",
    "\n",
    "    track_graphs = network_topology(voxels, clusts, edge_index[edge_mask], clust_labels=group_reco, edge_labels=edge_scores[edge_mask], colorscale=get_colorscale())\n",
    "    track_graphs[0]['name'] = 'Tracks (Reco)'\n",
    "    if len(track_graphs) == 2:\n",
    "        track_graphs[1]['name'] = 'Track edges (Reco)'\n",
    "    graphs += track_graphs\n",
    "\n",
    "# Particle groups\n",
    "if 'particle_edge_pred' in output:\n",
    "    clusts         = output['particle_fragments'][entry]\n",
    "#     print(output['fragments_seg'])\n",
    "    edge_index     = output['particle_edge_index'][entry]\n",
    "    group_label    = np.unique(get_cluster_label(torch.tensor(data['cluster_label'][entry]), clusts, column=6), return_inverse=True)[1]\n",
    "    edge_label     = group_label[edge_index[:,0]] == group_label[edge_index[:,1]]\n",
    "\n",
    "    part_graphs = network_topology(voxels, clusts, edge_index[edge_label], clust_labels=group_label, colorscale=get_colorscale())\n",
    "    part_graphs[0]['name'] = 'Particle groups (Label)'\n",
    "    if len(part_graphs) == 2:\n",
    "        part_graphs[1]['name'] = 'Particle edges (Label)'\n",
    "    graphs += part_graphs\n",
    "\n",
    "    edge_scores    = softmax(output['particle_edge_pred'][entry], axis=1)[:,1]\n",
    "#     print(len(edge_index))\n",
    "#     print(np.hstack((edge_index, edge_scores.reshape(-1,1), edge_label.reshape(-1,1))))\n",
    "#     print(output['particle_node_accuracy'])\n",
    "#     print(output['particle_edge_accuracy'])\n",
    "    edge_mask      = edge_scores > 0.5\n",
    "    edge_pred, _   = edge_assignment_score(edge_index, output['particle_edge_pred'][entry], len(clusts))\n",
    "    primary_scores = softmax(output['particle_node_pred'][entry], axis=1)[:,1]\n",
    "    group_reco     = output['particle_group_pred'][entry]\n",
    "\n",
    "    graphs += network_topology(voxels, clusts, clust_labels=primary_scores, colorscale='Portland', cmin=0, cmax=1)\n",
    "    graphs[-1]['name'] = 'Particle primaries (Reco)'\n",
    "\n",
    "    part_graphs = network_topology(voxels, clusts, edge_index[edge_mask], clust_labels=group_reco, edge_labels=edge_scores[edge_mask], colorscale=get_colorscale())\n",
    "    part_graphs[0]['name'] = 'Particles (Reco)'\n",
    "    if len(part_graphs) == 2:\n",
    "        part_graphs[1]['name'] = 'Particle edges (Reco)'\n",
    "    graphs += part_graphs\n",
    "\n",
    "# Interactions\n",
    "if 'inter_edge_pred' in output:\n",
    "    clusts         = output['inter_particles'][entry]\n",
    "    edge_index     = output['inter_edge_index'][entry]\n",
    "    group_label    = np.unique(get_cluster_label(torch.tensor(data['cluster_label'][entry]), clusts, column=7), return_inverse=True)[1]\n",
    "    edge_label     = group_label[edge_index[:,0]] == group_label[edge_index[:,1]]\n",
    "    pid_label      = get_cluster_label(torch.tensor(data['cluster_label'][entry]), clusts, column=9)\n",
    "\n",
    "    graphs += network_topology(voxels, clusts, clust_labels=pid_label, colorscale=get_colorscale(), cmin=-1, cmax=4)\n",
    "    graphs[-1]['name'] = 'Particle ID (Label)'\n",
    "\n",
    "    inter_graphs = network_topology(voxels, clusts, edge_index[edge_label], clust_labels=group_label, colorscale=get_colorscale())\n",
    "    inter_graphs[0]['name'] = 'Interactions (Label)'\n",
    "    if len(inter_graphs) == 2:\n",
    "        inter_graphs[1]['name'] = 'Interaction edges (Label)'\n",
    "    graphs += inter_graphs\n",
    "\n",
    "    edge_scores    = softmax(output['inter_edge_pred'][entry], axis=1)[:,1]\n",
    "    edge_mask      = edge_scores > 0.5\n",
    "    edge_pred, _   = edge_assignment_score(edge_index, output['inter_edge_pred'][entry], len(clusts))\n",
    "    pid_reco       = np.argmax(output['inter_node_pred'][entry], axis=1)\n",
    "    group_reco     = output['inter_group_pred'][entry]\n",
    "\n",
    "    print(pid_label)\n",
    "    print(pid_reco)\n",
    "    print(output['inter_node_loss'])\n",
    "    print(output['inter_node_accuracy'])\n",
    "\n",
    "    graphs += network_topology(voxels, clusts, clust_labels=pid_reco, colorscale=get_colorscale(), cmin=-1, cmax=4)\n",
    "    graphs[-1]['name'] = 'Particle ID (Reco)'\n",
    "\n",
    "    inter_graphs = network_topology(voxels, clusts, edge_index[edge_mask], clust_labels=group_reco, edge_labels=edge_scores[edge_mask], colorscale=get_colorscale())\n",
    "    inter_graphs[0]['name'] = 'Interactions (Reco)'\n",
    "    if len(inter_graphs) == 2:\n",
    "        inter_graphs[1]['name'] = 'Interaction edges (Reco)'\n",
    "    graphs += inter_graphs\n",
    "\n",
    "# DRAW\n",
    "iplot(go.Figure(graphs, layout=layout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*================================================================================================*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'notebook_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-89a923f4fc9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Define the data loader configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnotebook_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_inference_cfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_inference_cfg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train/pi0_full_pid_v01.cfg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trainval'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_path'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train/weights/full_pid/snapshot-139999.ckpt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'notebook_utils'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../lartpc_mlreco3d\")\n",
    "\n",
    "import yaml\n",
    "from mlreco.main_funcs import process_config, prepare\n",
    "\n",
    "# Define the data loader configuration\n",
    "from notebook_utils import get_inference_cfg\n",
    "cfg = get_inference_cfg(cfg_path = 'train/pi0_full_pid_v01.cfg', batch_size=16)\n",
    "cfg['trainval']['model_path'] = 'train/weights/full_pid/snapshot-139999.ckpt'\n",
    "cfg['iotool']['dataset']['data_keys'] = ['/home/frans/slac/dlp/data/pi0_dunend_v2_p00.root']\n",
    "#cfg['iotool']['dataset']['data_keys'] = ['/home/frans/slac/dlp/data/cluster_new/train.root']\n",
    "#data_name = cfg['model']['modules']['grappa_shower']['model_path'] = 'train/weights/full_pid/snapshot-100499.ckpt'\n",
    "data_name = cfg['iotool']['dataset']['data_keys'][0].split('/')[-1].split('.')[0]\n",
    "\n",
    "# Pre-process configuration\n",
    "process_config(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import softmax\n",
    "from scipy.spatial.distance import cdist\n",
    "from mlreco.utils.ppn import uresnet_ppn_type_point_selector\n",
    "from mlreco.models.chain.full_cnn import fit_predict, gaussian_kernel\n",
    "from mlreco.utils.gnn.cluster import get_cluster_label\n",
    "from mlreco.utils.gnn.evaluation import clustering_metrics, node_assignment_score\n",
    "from mlreco.utils.metrics import ARI, AMI, SBD, purity, efficiency\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('error', category=UserWarning)\n",
    "\n",
    "# Initialize data objects\n",
    "sem_data = {}\n",
    "sem_tags = ['event', 'label', 'pred', 'score_0', 'score_1', 'score_2', 'score_3', 'score_4']\n",
    "for tag in sem_tags:\n",
    "    sem_data[tag] = []\n",
    "    \n",
    "ppn_eff_data = {'event':[], 'dist':[], 'class':[]}\n",
    "ppn_pur_data = {'event':[], 'dist':[], 'class':[]}\n",
    "\n",
    "frag_data = {'event':[]}\n",
    "frag_tags = ['ari', 'ami', 'sbd', 'pur', 'eff']\n",
    "frag_funcs = {'ari':ARI, 'ami':AMI,'sbd':SBD,'pur':purity,'eff':efficiency}\n",
    "for tag in frag_tags:\n",
    "    frag_data[tag] = []\n",
    "    for i in range(4):\n",
    "        frag_data[f'{tag}_{i}'] = []\n",
    "\n",
    "part_data = {'event':[], 'primary_acc':[]}\n",
    "for tag in frag_tags:\n",
    "    part_data[tag] = []\n",
    "    \n",
    "inter_data = {'event':[], 'num_inter':[]}\n",
    "for tag in frag_tags:\n",
    "    inter_data[tag] = []\n",
    "    \n",
    "pid_data = {}\n",
    "pid_tags = ['event', 'label', 'pred', 'score_0', 'score_1', 'score_2', 'score_3', 'score_4']\n",
    "for tag in pid_tags:\n",
    "    pid_data[tag] = []\n",
    "    \n",
    "# Instantiate \"handlers\" (IO tools)\n",
    "from mlreco.main_funcs import prepare\n",
    "hs = prepare(cfg)\n",
    "\n",
    "# Loop over the requested amount of events\n",
    "batch_size = int(cfg['iotool']['batch_size'])\n",
    "it = iter(hs.data_io)\n",
    "print('Number of batches in dataset: {} ({} events)'.format(len(hs.data_io), len(hs.data_io)*batch_size))\n",
    "size = len(hs.data_io)\n",
    "size = 100\n",
    "for i in range(size):\n",
    "    \n",
    "    # Get forward output\n",
    "    print('Batch',i)\n",
    "    if i%100 == 0: print('Processed {} batches ({} events)'.format(i, i*batch_size))\n",
    "    data, output = hs.trainer.forward(hs.data_io_iter)\n",
    "    \n",
    "    # Prepare the semantics label tensor for the PPN point predictor\n",
    "    ppn_sem_data = np.vstack(data['input_data'])\n",
    "    #ppn_sem_data[:,4] = np.vstack(data['cluster_label'])[:,-1]\n",
    "    ppn_sem_data[:,4] = np.argmax(np.vstack(output['segmentation']), axis=1)\n",
    "    ppn_sem_data = [ppn_sem_data[ppn_sem_data[:,3] == b] for b in np.unique(ppn_sem_data[:,3])]\n",
    "        \n",
    "    for b in range(len(data['input_data'])):\n",
    "        # Get the event ID\n",
    "        event_id = data['index'][b]\n",
    "        \n",
    "        # Append the semantic prediction data\n",
    "        sem_data['event'].extend(np.full(len(data['cluster_label'][b]), event_id))\n",
    "        sem_data['label'].extend(data['cluster_label'][b][:,-1])\n",
    "        sem_data['pred'].extend(np.argmax(output['segmentation'][b], axis=1))\n",
    "        for s in range(5):\n",
    "            sem_data[f'score_{s}'].extend(output['segmentation'][b][:,s])\n",
    "            \n",
    "        # Append the point prediction data\n",
    "        label_points = data['particles_label'][b][:,:3]\n",
    "        label_classes = data['particles_label'][b][:,-2].astype(int)\n",
    "        pred_points = uresnet_ppn_type_point_selector(\n",
    "            ppn_sem_data[b], output, score_threshold=0.9, type_score_threshold=0.0, type_threshold=1.999,\n",
    "            entry=b, score_pool='max')\n",
    "            #ppn_sem_data[b], output, score_threshold=0.5, type_score_threshold=0.5, type_threshold=1.999,\n",
    "            #entry=b, score_pool='max')\n",
    "        pred_classes = pred_points[:,-1].astype(int)\n",
    "        dist_mat = cdist(label_points, pred_points[:,:3])\n",
    "        \n",
    "        ppn_eff_data['event'].extend(np.full(dist_mat.shape[0], event_id))\n",
    "        ppn_eff_data['dist'].extend(np.min(dist_mat, axis=1))\n",
    "        ppn_eff_data['class'].extend(label_classes)\n",
    "        ppn_pur_data['event'].extend(np.full(dist_mat.shape[1], event_id))\n",
    "        ppn_pur_data['dist'].extend(np.min(dist_mat, axis=0))\n",
    "        ppn_pur_data['class'].extend(pred_classes)\n",
    "        \n",
    "        # Append the fragment prediction data\n",
    "        frag_data['event'].append(event_id)\n",
    "        frag_label = data['cluster_label'][b][:,5]\n",
    "        sem_pred = np.argmax(output['segmentation'][b], axis=1)\n",
    "        sem_label = data['cluster_label'][b][:,-1]\n",
    "        part_sem = np.array([sem_pred[p[0]] for p in output['particles'][b]])\n",
    "        fragments = np.append(output['shower_fragments'][b], output['particles'][b][part_sem != 0])\n",
    "        frag_pred = -np.ones(len(frag_label))\n",
    "        for k, f in enumerate(fragments):\n",
    "            frag_pred[f] = k\n",
    "        for s in range(4):\n",
    "            sem_mask = (sem_pred == s) & (sem_label == s)\n",
    "            for tag in frag_tags:\n",
    "                if np.sum(sem_mask):\n",
    "                    frag_data['{}_{}'.format(tag,s)].append(frag_funcs[tag](frag_label[sem_mask], frag_pred[sem_mask]))\n",
    "                else:\n",
    "                    frag_data['{}_{}'.format(tag,s)].append(-1)\n",
    "            \n",
    "        he_mask = (sem_pred < 4) & (sem_label < 4)\n",
    "        for tag in frag_tags:\n",
    "            frag_data[tag].append(frag_funcs[tag](frag_label[he_mask], frag_pred[he_mask]))\n",
    "            \n",
    "        # Append the shower clustering data\n",
    "        part_data['event'].append(event_id)\n",
    "        fragments = output['shower_fragments'][b]\n",
    "        sem_mask = (sem_pred == 0) & (sem_label == 0)\n",
    "        if len(fragments) and np.sum(sem_mask):\n",
    "            # Cluster-wise metrics\n",
    "#             group_pred = output['shower_group_pred'][b]\n",
    "#             true_groups = get_cluster_label(torch.tensor(data['cluster_labels'][b]), fragments, column=6)\n",
    "#             metrics = clustering_metrics(fragments, true_groups, group_pred)\n",
    "#             for k, tag in enumerate(frag_tags):\n",
    "#                 part_data[tag].append(metrics[k])\n",
    "\n",
    "            # Voxel-wise metrics\n",
    "            group_label = data['cluster_label'][b][:,6]\n",
    "            group_ids = output['shower_group_pred'][b]\n",
    "            group_pred = -np.ones(len(group_label))\n",
    "            for k, f in enumerate(fragments):\n",
    "                group_pred[f] = group_ids[k]\n",
    "            for tag in frag_tags:\n",
    "                part_data[tag].append(frag_funcs[tag](group_label[sem_mask], group_pred[sem_mask]))\n",
    "                \n",
    "            part_data['primary_acc'].append(output['shower_node_accuracy'][0])\n",
    "            all_acc = np.array(part_data['primary_acc'])\n",
    "#             print('acc:', all_acc[-1], all_acc[all_acc > 0].mean(), len(part_data['event']), len(part_data['primary_acc']))\n",
    "                \n",
    "#             # Restrict to clusters in PREDICTED groups that have a SINGLE true primary\n",
    "#             node_scores = softmax(output['frag_node_pred'][b], axis=1)\n",
    "#             node_preds = np.argmax(node_scores, axis=1)\n",
    "\n",
    "#             labels = data['cluster_label'][b]\n",
    "#             group_ids = get_cluster_label(torch.tensor(labels), fragments, column=6)\n",
    "#             pred_group_ids = output['frag_group_pred'][b]\n",
    "#             clust_ids = get_cluster_label(torch.tensor(labels), fragments)\n",
    "#             primary_labels = clust_ids == group_ids\n",
    "#             purity_mask = np.zeros(len(group_ids), dtype=bool)\n",
    "#             for g in np.unique(pred_group_ids):\n",
    "#                 mask = np.where(pred_group_ids == g)[0]\n",
    "#                 print('group', event_id, g, mask, primary_labels[mask])\n",
    "#                 if np.sum(primary_labels[mask]) == 1:\n",
    "#                     purity_mask[mask] = np.ones(len(mask), dtype=bool)\n",
    "#             print('purity_mask', purity_mask)\n",
    "\n",
    "#             # Only pick a single cluster as primary per predicted group\n",
    "#             scores_primary = node_scores[purity_mask,1]\n",
    "#             clusts = fragments[purity_mask]\n",
    "#             scores = node_scores[purity_mask]\n",
    "#             clust_ids = clust_ids[purity_mask]\n",
    "#             group_ids = output['frag_group_pred'][b][purity_mask]\n",
    "#             preds = np.zeros(len(group_ids))\n",
    "#             truths = clust_ids == group_ids\n",
    "#             for g in np.unique(group_ids):\n",
    "#                 mask = np.where(group_ids == g)[0]\n",
    "#                 max_id = mask[np.argmax(scores_primary[mask])]\n",
    "#                 preds[max_id] = 1\n",
    "\n",
    "#             if len(np.unique(group_ids)) != np.sum(preds):\n",
    "#                 print(len(group_ids), np.sum(preds))\n",
    "\n",
    "            # Evaluate node-wise primary accuracy\n",
    "#             part_data['primary_acc'].append(np.sum(truths==preds)/len(preds))\n",
    "        else:\n",
    "            for k, tag in enumerate(frag_tags):\n",
    "                part_data[tag].append(-1)\n",
    "            part_data['primary_acc'].append(-1)\n",
    "            \n",
    "        # Append the interaction clustering data\n",
    "        inter_data['event'].append(event_id)\n",
    "        particles = output['particles'][b]\n",
    "        sem_mask = (sem_pred < 4) & (sem_label < 4)\n",
    "        if len(particles) and np.sum(sem_mask):\n",
    "            # Cluster-wise metrics\n",
    "#             group_pred = node_assignment_score(output['inter_edge_index'][b], output['inter_edge_pred'][b], len(particles))\n",
    "#             true_groups = get_cluster_label(torch.tensor(data['cluster_labels'][b]), particles, column=7)\n",
    "#             metrics = clustering_metrics(particles, true_groups, group_pred)\n",
    "#             for k, tag in enumerate(frag_tags):\n",
    "#                 inter_data[tag].append(metrics[k])\n",
    "                \n",
    "            # Voxel-wise metrics\n",
    "            group_label = data['cluster_label'][b][:,7]\n",
    "            num_inter = len(np.unique(group_label[sem_mask]))\n",
    "            inter_data['num_inter'].append(num_inter)\n",
    "            group_ids = node_assignment_score(output['inter_edge_index'][b], output['inter_edge_pred'][b], len(particles))\n",
    "            group_pred = -np.ones(len(group_label))\n",
    "            for k, p in enumerate(particles):\n",
    "                group_pred[p] = group_ids[k]\n",
    "            for tag in frag_tags:\n",
    "                inter_data[tag].append(frag_funcs[tag](group_label[sem_mask], group_pred[sem_mask]))\n",
    "        else:\n",
    "            for k, tag in enumerate(frag_tags):\n",
    "                inter_data[tag].append(-1)\n",
    "                \n",
    "        # Append the particle type prediction data\n",
    "        pid_true   = get_cluster_label(torch.tensor(data['cluster_label'][b]), output['inter_particles'][b], column=9)\n",
    "        pid_mask   = np.where(pid_true > -1)[0]\n",
    "        pid_true   = pid_true[pid_mask]\n",
    "        pid_scores = softmax(output['inter_node_pred'][b][pid_mask], axis=1)\n",
    "        pid_pred   = np.argmax(pid_scores, axis=1)\n",
    "        pid_data['event'].extend(np.full(len(pid_true), event_id))\n",
    "        pid_data['label'].extend(pid_true)\n",
    "        pid_data['pred'].extend(pid_pred)\n",
    "        for s in range(5):\n",
    "            pid_data[f'score_{s}'].extend(pid_scores[:,s])\n",
    "        \n",
    "    \n",
    "# Initialize dataframe, save to CSV\n",
    "dir_path = 'inference'\n",
    "pd.DataFrame(sem_data).to_csv(f'{dir_path}/semantics_{data_name}.csv', index=False)\n",
    "pd.DataFrame(ppn_eff_data).to_csv(f'{dir_path}/ppn_efficiency_{data_name}.csv', index=False)\n",
    "pd.DataFrame(ppn_pur_data).to_csv(f'{dir_path}/ppn_purity_{data_name}.csv', index=False)\n",
    "pd.DataFrame(frag_data).to_csv(f'{dir_path}/fragments_{data_name}.csv', index=False)\n",
    "pd.DataFrame(part_data).to_csv(f'{dir_path}/shower_{data_name}.csv', index=False)\n",
    "pd.DataFrame(inter_data).to_csv(f'{dir_path}/inter_{data_name}.csv', index=False)\n",
    "pd.DataFrame(pid_data).to_csv(f'{dir_path}/pid_{data_name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "import seaborn\n",
    "\n",
    "seaborn.set(rc={'figure.figsize':(12,9)}, context='notebook')\n",
    "seaborn.set_style('white')\n",
    "seaborn.set_style(rc={'axes.grid':True})\n",
    "\n",
    "dir_path  = 'inference'\n",
    "#data_name = 'test'\n",
    "data_name = 'pi0_dunend_v2_p00'\n",
    "\n",
    "# Bin the predicted semantic class as a function of the true semantic class\n",
    "df = pd.read_csv(f'{dir_path}/semantics_{data_name}.csv')\n",
    "hist, xbins, ybins = np.histogram2d(df.pred, df.label, range=[[-0.5,4.5],[-0.5,4.5]], bins=[5,5])\n",
    "norms = np.sum(hist, axis=0)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.pcolormesh(xbins, ybins, hist/norms, cmap='Blues')\n",
    "for i in range(len(ybins)-1):\n",
    "    for j in range(len(xbins)-1):\n",
    "        plt.text(xbins[j]+0.5,ybins[i]+0.5, '{:0.3f}'.format(hist[i,j]/norms[j]), \n",
    "                color=\"white\" if i==j else \"black\", ha=\"center\", va=\"center\")\n",
    "plt.xlabel('Class label')\n",
    "plt.ylabel('Class prediction')\n",
    "plt.xticks([0,1,2,3,4], labels=['Shower','Track','Michel','Delta','LE'])\n",
    "plt.yticks([0,1,2,3,4], labels=['Shower','Track','Michel','Delta','LE'])\n",
    "plt.colorbar()\n",
    "plt.savefig(f'{dir_path}/semantics_confmat_{data_name}.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Plot efficiency and purity of point prediction\n",
    "df_pur = pd.read_csv(f'{dir_path}/ppn_purity_{data_name}.csv')\n",
    "df_eff = pd.read_csv(f'{dir_path}/ppn_efficiency_{data_name}.csv')\n",
    "ppn_eff = np.sum(df_eff[df_eff['class'] < 3].dist<10)/len(df_eff[df_eff['class'] < 3])\n",
    "ppn_pur = np.sum(df_pur[df_pur['class'] < 3].dist<10)/len(df_pur[df_pur['class'] < 3])\n",
    "plt.hist(df_pur.dist[df_pur['class'] < 3], alpha=.5, bins=50, range=[0,10], label=f'True to closest prediction\\nPurity: {ppn_pur:0.3f}')\n",
    "plt.hist(df_eff[df_eff['class'] < 3].dist, alpha=.5, bins=50, range=[0,10], label=f'Prediction to closest true\\nEfficiency: {ppn_eff:0.3f}')\n",
    "plt.xlabel('Distance [voxel]')\n",
    "plt.ylabel('# of points')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.savefig(f'{dir_path}/ppn_dist_{data_name}.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Plot clustering metrics of fragment prediction\n",
    "df = pd.read_csv(f'{dir_path}/fragments_{data_name}.csv')\n",
    "plt.hist(df.ari_0[df.ari_0>0], alpha=.5, bins=50, range=[0,1.00001], label='Shower')\n",
    "plt.hist(df.ari_1[df.ari_1>0], alpha=.5, bins=50, range=[0,1.00001], label='Track')\n",
    "plt.hist(df.ari_2[df.ari_2>0], alpha=.5, bins=50, range=[0,1.00001], label='Michel')\n",
    "plt.hist(df.ari_3[df.ari_3>0], alpha=.5, bins=50, range=[0,1.00001], label='Delta')\n",
    "plt.xlabel('Metric')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.savefig(f'{dir_path}/frag_ari_{data_name}.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.boxplot([df.ari_0[df.ari_0>0],df.ari_1[df.ari_1>0],df.ari_2[df.ari_2>0],df.ari_3[df.ari_3>0]], showmeans=True,showfliers=False, labels=['Shower','Track','Michel','Delta'])\n",
    "plt.ylabel('ARI')\n",
    "plt.savefig(f'{dir_path}/frag_ari_boxplot_{data_name}.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Plot clustering metrics of shower grouping\n",
    "df = pd.read_csv(f'{dir_path}/shower_{data_name}.csv')\n",
    "plt.hist(df.ari, alpha=.5, bins=50, range=[0,1.00001], label='ARI (Mean: {:0.3f})'.format(df.ari[df.ari>0].mean()))\n",
    "plt.hist(df.pur, alpha=.5, bins=50, range=[0,1.00001], label='Purity (Mean: {:0.3f})'.format(df.pur[df.pur>-1].mean()))\n",
    "plt.hist(df.eff, alpha=.5, bins=50, range=[0,1.00001], label='Efficiency (Mean: {:0.3f})'.format(df.eff[df.eff>-1].mean()))\n",
    "plt.xlabel('Metric')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.savefig(f'{dir_path}/shower_grouping_metrics_{data_name}.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Plot clustering metrics of shower grouping\n",
    "df = pd.read_csv(f'{dir_path}/inter_{data_name}.csv')\n",
    "plt.hist(df.ari, alpha=.5, bins=50, range=[0,1.00001], label='ARI (Mean: {:0.3f})'.format(df.ari[df.ari>0].mean()))\n",
    "plt.hist(df.pur, alpha=.5, bins=50, range=[0,1.00001], label='Purity (Mean: {:0.3f})'.format(df.pur[df.pur>-1].mean()))\n",
    "plt.hist(df.eff, alpha=.5, bins=50, range=[0,1.00001], label='Efficiency (Mean: {:0.3f})'.format(df.eff[df.eff>-1].mean()))\n",
    "plt.xlabel('Metric')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.savefig(f'{dir_path}/inter_grouping_metrics_{data_name}.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Bin the predicted semantic class as a function of the true semantic class\n",
    "df = pd.read_csv(f'{dir_path}/pid_{data_name}.csv')\n",
    "hist, xbins, ybins = np.histogram2d(df.pred, df.label, range=[[-0.5,4.5],[-0.5,4.5]], bins=[5,5])\n",
    "norms = np.sum(hist, axis=0)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.pcolormesh(xbins, ybins, hist/norms, cmap='Blues')\n",
    "for i in range(len(ybins)-1):\n",
    "    for j in range(len(xbins)-1):\n",
    "        plt.text(xbins[j]+0.5,ybins[i]+0.5, '{:0.3f}'.format(hist[i,j]/norms[j]), \n",
    "                color=\"white\" if i==j else \"black\", ha=\"center\", va=\"center\")\n",
    "plt.xlabel('Particle label')\n",
    "plt.ylabel('Particle prediction')\n",
    "plt.xticks([0,1,2,3,4], labels=['Photon','Electron','Muon','Pion','Proton'])\n",
    "plt.yticks([0,1,2,3,4], labels=['Photon','Electron','Muon','Pion','Proton'])\n",
    "plt.colorbar()\n",
    "plt.savefig(f'{dir_path}/pid_confmat_{data_name}.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{dir_path}/semantics_{data_name}.csv')\n",
    "for e in np.unique(df.event):\n",
    "    n_tracks = len(df[(df.event == e) & (df.label == 1)])\n",
    "    if n_tracks:\n",
    "        ratio = len(df[(df.event == e) & (df.label == 1) & (df.pred == 3)])/n_tracks\n",
    "        if ratio > 0.03:\n",
    "            print(e, n_tracks, ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{dir_path}/pid_{data_name}.csv')\n",
    "for e in np.unique(df.event):\n",
    "    n_tracks = len(df[(df.event == e) & (df.label == 1)])\n",
    "    if n_tracks:\n",
    "        print('>', e)\n",
    "        if len(df[(df.event == e) & (df.label == 1) & (df.pred == 0)]):\n",
    "            print(e)\n",
    "df[(df.label == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{dir_path}/pid_{data_name}.csv')\n",
    "for e in np.unique(df.event):\n",
    "    n_tracks = len(df[(df.event == e) & (df.label == 0)])\n",
    "    if n_tracks:\n",
    "        if len(df[(df.event == e) & (df.label == 0) & (df.pred == 1)]):\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dir_path = 'inference'\n",
    "#data_name = 'test'\n",
    "data_name = 'pi0_dunend_v2_p00'\n",
    "df = pd.read_csv(f'{dir_path}/shower_{data_name}.csv')\n",
    "print(len(df))\n",
    "print(np.mean(df.primary_acc[df.primary_acc>0]))\n",
    "df[(df.ari < 0.9) & (df.ari >= 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'pi0_dunend_v2_p00'\n",
    "df = pd.read_csv(f'{dir_path}/fragments_{data_name}.csv')\n",
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{dir_path}/pid_{data_name}.csv')\n",
    "df[(df.label == 2) & (df.pred == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import numpy as np\n",
    "# from scipy.special import softmax\n",
    "# from scipy.spatial.distance import cdist\n",
    "# from mlreco.utils.gnn.cluster import get_cluster_label\n",
    "# from mlreco.visualization.gnn import network_topology\n",
    "# voxels = data['cluster_label'][0][:,:3]\n",
    "# semantics = np.argmax(output['segmentation'][0], axis=1)\n",
    "\n",
    "# # Draw shower fragment start points\n",
    "# fragments = output['shower_fragments'][0]\n",
    "# frag_points = np.empty((0, 3))\n",
    "# for f in fragments:\n",
    "#     dmask = np.where(np.max(np.abs(output['points'][0][f][:,:3]),axis=1) < 1)[0] # Make sure point is part of fragment\n",
    "#     scores = softmax(output['points'][0][f][:,3:5], axis=1)\n",
    "#     argmax = dmask[np.argmax(scores[dmask,-1])] if len(dmask) else np.argmax(scores[:,-1])\n",
    "#     start  = data['cluster_label'][0][f][argmax,:3]+output['points'][0][f][argmax,:3]+0.5\n",
    "#     frag_points = np.concatenate((frag_points, start.reshape(1,3)), axis=0)\n",
    "\n",
    "# particles = output['particles'][0]\n",
    "# part_sem = output['particles_seg'][0]\n",
    "# node_scores = softmax(output['shower_node_pred'][0], axis=1)\n",
    "# group_pred = np.unique(output['shower_group_pred'][0], return_inverse=True)[1]\n",
    "# part_points = np.empty((0, 3))\n",
    "# for i, p in enumerate(particles):\n",
    "#     if part_sem[i] == 1:\n",
    "#         dist_mat = cdist(data['cluster_label'][0][p,:3], data['cluster_label'][0][p,:3])\n",
    "#         idx = np.argmax(dist_mat)\n",
    "#         start_id, end_id = int(idx/len(p)), int(idx%len(p))\n",
    "#         start, end = data['cluster_label'][0][p[start_id],:3], data['cluster_label'][0][p[end_id],:3]\n",
    "#         part_points = np.concatenate((part_points, start.reshape(1,3)), axis=0)\n",
    "#         part_points = np.concatenate((part_points, end.reshape(1,3)), axis=0)   \n",
    "#     else:\n",
    "#         if part_sem[i] == 0:\n",
    "#             part_mask = np.where(group_pred == i)[0]\n",
    "#             idx = np.argmax(node_scores[part_mask,1])\n",
    "#             #print(i, part_mask, node_scores[part_mask,1], idx, part_mask[idx])\n",
    "#             p = fragments[part_mask[idx]]\n",
    "#         dmask = np.where(np.max(np.abs(output['points'][0][p][:,:3]),axis=1) < 1)[0] # Make sure point is part of fragment\n",
    "#         scores = softmax(output['points'][0][p][:,3:5], axis=1)\n",
    "#         argmax = dmask[np.argmax(scores[dmask,-1])] if len(dmask) else np.argmax(scores[:,-1])\n",
    "#         start  = data['cluster_label'][0][p][argmax,:3] + output['points'][0][p][argmax,:3] + 0.5\n",
    "#         part_points = np.concatenate((part_points, start.reshape(1,3)), axis=0)\n",
    "        \n",
    "\n",
    "# graph_frag = network_topology(voxels, fragments, clust_labels=np.arange(len(fragments)), markersize=3, colorscale=get_colorscale())\n",
    "# graph_part = network_topology(voxels, particles, clust_labels=np.arange(len(particles)), markersize=3, colorscale=get_colorscale())\n",
    "# graph_points_frag = scatter_points(frag_points, color='gray', markersize=7)\n",
    "# graph_points_frag[0]['name'] = 'Fragment starts'\n",
    "# graph_points_part = scatter_points(part_points, color='cyan', markersize=7)\n",
    "# graph_points_part[0]['name'] = 'Particle ends'\n",
    "# iplot(go.Figure([*graph_frag, *graph_points_frag], layout=layout))\n",
    "# iplot(go.Figure([*graph_part, *graph_points_frag, *graph_points_part], layout=layout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "class ShowerIdentifier():\n",
    "\n",
    "    def __init__(self, **cfg):\n",
    "        '''\n",
    "        Extracts the shower idenfication parameters from the module configuration\n",
    "        '''\n",
    "        self._max_distance  = cfg.get('max_distance', 7)     # Maximum distance from the shower start over which to integrate the energy\n",
    "        self._voxel_size    = cfg.get('voxel_size', 0.3)     # Image voxel size in cm\n",
    "\n",
    "        self._e_scale       = cfg.get('e_scale', 1.)         # Prior knowledge of the electron fraction in the sample of showers\n",
    "        self._e_shift       = cfg.get('e_shift', 9.710)      # Peak of the electron dEdx distribution\n",
    "        self._e_squeeze     = cfg.get('e_squeeze', 4.961)    # Inverse width (?) of the electron dEdx distribution\n",
    "\n",
    "        self._p_scale       = cfg.get('p_scale', 1.)         # Prior knowledge of the electron fraction in the sample of showers\n",
    "        self._p_shift       = cfg.get('p_shift', 10.233)     # Peak of the electron dEdx distribution\n",
    "        self._p_squeeze     = cfg.get('p_squeeze', 2.542)    # Inverse width (?) of the electron dEdx distribution\n",
    "\n",
    "\n",
    "    def likelihood_fractions(self, showers, energy):\n",
    "        '''\n",
    "        Obtain the electron- and photon likelihood fractions for the showers by looking at\n",
    "        the dE/dx value at the very start of an EM shower.\n",
    "\n",
    "        Inputs:\n",
    "            - showers (M x 1): Array of M shower objects (defined in chain.py)\n",
    "            - energy (N x 5): All energy deposits (x,y,z,batch_id,edep) which have semantic segmentation 'shower'\n",
    "        Returns:\n",
    "            - L_electron (M x 1): Array of M electron likelihood fractions (one likelihood fraction per showers)\n",
    "            - L_photon (M x 1): Array of M photon likelihood fraction (one likelihood fraction per showers)\n",
    "        '''\n",
    "\n",
    "        # Loop over all shower objects, sum up the energy depositions within some distance\n",
    "        # and calculate the likelihood fractions from the dEdx distributions.\n",
    "        for sh_index, sh in enumerate(showers):\n",
    "\n",
    "            # Find distance from start to all other shower voxels\n",
    "            coords = energy[sh.voxels,:3]\n",
    "            dists  = cdist(sh.start.reshape(1,3), coords).flatten()\n",
    "\n",
    "            # Sum all energies within the maximum allowed distance, convert to dEdx\n",
    "            edeps      = energy[sh.voxels, -1]\n",
    "            total_edep = np.sum(edeps[dists < self._max_distance])\n",
    "            dEdx       = total_edep / (self._max_distance * self._voxel_size)\n",
    "\n",
    "            # Obtain likelihoods\n",
    "            e_likelihood = self.moyal(dEdx, 'e')\n",
    "            p_likelihood = self.moyal(dEdx, 'p')\n",
    "\n",
    "            # Obtain likelihood fractions\n",
    "            if (e_likelihood + p_likelihood) > 0:\n",
    "                sh.L_e = e_likelihood / (e_likelihood + p_likelihood)\n",
    "                sh.L_p = p_likelihood / (e_likelihood + p_likelihood)\n",
    "            else:\n",
    "                sh.L_e = float(dEdx < self._e_shift/self._e_squeeze)\n",
    "                sh.L_p = float(dEdx > self._p_shift/self._p_squeeze)\n",
    "\n",
    "\n",
    "    def moyal(self, dEdx, type):\n",
    "        '''\n",
    "        Computes the value of the Moyal distribution (approx. to Landau) at a specific dEdx\n",
    "        '''\n",
    "        a, b, c = getattr(self, f'_{type}_scale'), getattr(self, f'_{type}_shift'), getattr(self, f'_{type}_squeeze')\n",
    "        print(type, dEdx, a * 1./(np.sqrt(2.*np.pi)) * np.exp(-0.5*((c*dEdx-b)+np.exp(-(c*dEdx-b)))))\n",
    "        return a * 1./(np.sqrt(2.*np.pi)) * np.exp(-0.5*((c*dEdx-b)+np.exp(-(c*dEdx-b))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier = ShowerIdentifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(identifier._e_shift/identifier._e_squeeze)\n",
    "print(identifier._p_shift/identifier._p_squeeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dEdx = 4\n",
    "\n",
    "identifier.moyal(dEdx, 'e')\n",
    "identifier.moyal(dEdx, 'p')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
